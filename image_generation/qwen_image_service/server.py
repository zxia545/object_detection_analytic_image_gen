import os
import uuid
import subprocess
from typing import Optional, Dict, Any

import torch
import asyncio
import requests
import psutil

from fastapi import FastAPI, BackgroundTasks, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import RedirectResponse
from pydantic import BaseModel
from contextlib import asynccontextmanager
from fastapi.responses import FileResponse
import uvicorn

import random
###############################################################################
# é…ç½®å‚æ•°
###############################################################################
MODEL_PATHS = ["/root/autodl-tmp/Qwen-Image", "/root/autodl-fs/Qwen-Image"]
DEFAULT_OUTPUT_DIR = "/root/autodl-tmp/output_images"
MIN_FREE_GB = 5  # å¦‚æœå¯ç”¨ç©ºé—´å°äº10GBåˆ™æ¸…ç†

positive_magic = {
    "en": "",
    "zh": " è¶…æ¸…ï¼Œ4Kï¼Œç”µå½±çº§æ„å›¾",
}
aspect_ratios = {
    "1:1": (1328, 1328),
    "16:9": (1664, 928),
    "9:16": (928, 1664),
    "4:3": (1472, 1140),
    "3:4": (1140, 1472),
}

###############################################################################
# å·¥å…·å‡½æ•°
###############################################################################
def make_sure_model():
    for path in MODEL_PATHS:
        if os.path.exists(path):
            print(f"Using model: {path}")
            return path
    # è‹¥å‡æ²¡æœ‰åˆ™è‡ªåŠ¨ä¸‹è½½
    print("Model not found. Running /root/download_model.sh ...")
    code = subprocess.call(['bash', '/root/download_model.sh'])
    # if code != 0:
    #     raise RuntimeError('/root/download_model.sh failed!')
    for path in MODEL_PATHS:
        if os.path.exists(path):
            print(f"Downloaded and using model: {path}")
            return path
    raise RuntimeError('Model still not found after download!')

def check_cuda_offload():  # å¤§äº30Gæ˜¾å­˜ä¸å¼€å¯offload
    if not torch.cuda.is_available():
        return True
    total_gb = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
    return total_gb < 30

def clean_old_images(output_dir):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        return
    disk = psutil.disk_usage(output_dir)
    freespace_gb = disk.free / (1024 ** 3)
    if freespace_gb < MIN_FREE_GB:
        files = [(f, os.path.getctime(os.path.join(output_dir, f)), os.path.getsize(os.path.join(output_dir, f))) 
                 for f in os.listdir(output_dir) if os.path.isfile(os.path.join(output_dir, f))]
        files.sort(key=lambda x: (x[2], x[1]), reverse=True)  # å¤§æ–‡ä»¶/æ—§æ–‡ä»¶ä¼˜å…ˆåˆ 
        for f, _, _ in files:
            try:
                os.remove(os.path.join(output_dir, f))
                print(f"Cleanup: deleted {f}")
            except:
                continue
            disk = psutil.disk_usage(output_dir)
            if disk.free / (1024 ** 3) >= MIN_FREE_GB:
                break

###############################################################################
# å…¨å±€çŠ¶æ€å˜é‡
###############################################################################
tasks: Dict[str, Dict[str, Any]] = {}
task_lock = asyncio.Lock()
pipe = None                       # pipeline å¯¹è±¡
model_loaded = False
pipe_lock = asyncio.Lock()        # æ¨ç†ä¸²è¡Œé”

###############################################################################
# Pydantic æ•°æ®æ¨¡å‹
###############################################################################
class GenerateRequest(BaseModel):
    prompt: str
    negative_prompt: Optional[str] = ""
    aspect_ratio: Optional[str] = "16:9"
    num_inference_steps: Optional[int] = 35
    true_cfg_scale: Optional[float] = 4.0
    seed: Optional[int] = 42
    language: Optional[str] = "en"
    callback_url: Optional[str] = None
    output_dir: Optional[str] = None  # æ–°å¢ï¼šæŒ‡å®šè¾“å‡ºç›®å½•

class GenerateStatusResponse(BaseModel):
    status: str
    result_url: Optional[str] = None
    detail: Optional[str] = None
    
class GenerateCaseRequest(BaseModel):
    test_case_id: str
    prompt: str
    negative_prompt: Optional[str] = ""
    aspect_ratio: Optional[str] = "16:9"
    num_inference_steps: Optional[int] = 10
    true_cfg_scale: Optional[float] = 4.0
    seed: Optional[int] = random.randint(0, 100)
    language: Optional[str] = "en"
    output_dir: Optional[str] = None  # æ–°å¢ï¼šæŒ‡å®šè¾“å‡ºç›®å½•
###############################################################################
# FastAPI/lifespan
###############################################################################
@asynccontextmanager
async def lifespan(app: FastAPI):
    model_path = make_sure_model()
    if not os.path.exists(DEFAULT_OUTPUT_DIR):
        os.makedirs(DEFAULT_OUTPUT_DIR)
    app.state.model_path = model_path
    yield
    # å¯åœ¨æ­¤é‡Šæ”¾æ¨¡å‹èµ„æº

app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)
app.mount("/static", StaticFiles(directory="static"), name="static")
###############################################################################
# Pipeline åŠ è½½ä¸ inference è°ƒç”¨å®ç°
###############################################################################
def safe_load_pipe(model_path, cpu_offload):
    from diffusers import DiffusionPipeline, QwenImageTransformer2DModel
    from transformers.modeling_utils import no_init_weights
    from dfloat11 import DFloat11Model

    with no_init_weights():
        transformer = QwenImageTransformer2DModel.from_config(
            QwenImageTransformer2DModel.load_config(
                "Qwen/Qwen-Image", subfolder="transformer",local_files_only=True
            ),local_files_only=True
        ).to(torch.bfloat16)
    
    DFloat11Model.from_pretrained(
        "/root/DFloat11__Qwen-Image-DF11",
        device="cpu",
        cpu_offload=cpu_offload,
        pin_memory=False,
        bfloat16_model=transformer,
        local_files_only=True
    )
    
    pipe = DiffusionPipeline.from_pretrained(
        "/root/autodl-tmp/Qwen-Image",
        transformer=transformer,
        torch_dtype=torch.bfloat16,
        # cache_dir="/root/autodl-tmp/Qwen-Image",
        local_files_only=True
    )
    pipe.enable_model_cpu_offload()
    return pipe

def _generate(
    prompt, negative_prompt, aspect_ratio, num_inference_steps, true_cfg_scale, seed, language, output_file, cpu_offload, model_path
):
    from torch import Generator
    global pipe, model_loaded

    # åªåŠ è½½ä¸€æ¬¡pipeline
    if not model_loaded or pipe is None:
        print("load model!")
        pipe = safe_load_pipe(model_path, cpu_offload)
        model_loaded = True
    width, height = aspect_ratios.get(aspect_ratio, (1664, 928))
    p_prompt = prompt + positive_magic.get(language, positive_magic["en"])
    with torch.inference_mode():
        image = pipe(
            prompt=p_prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            num_inference_steps=num_inference_steps,
            true_cfg_scale=true_cfg_scale,
            generator=Generator(device="cuda" if torch.cuda.is_available() else "cpu").manual_seed(seed)
        ).images[0]
        image.save(output_file)

###############################################################################
# API - 1. ç”Ÿæˆä»»åŠ¡æ¥å£ (å¼‚æ­¥/æ’é˜Ÿ)
###############################################################################

@app.post("/generate_case")
async def generate_case(req: GenerateCaseRequest, background_tasks: BackgroundTasks):
    # ç¡®å®šè¾“å‡ºç›®å½•å’Œæ–‡ä»¶è·¯å¾„
    output_dir = req.output_dir if req.output_dir else DEFAULT_OUTPUT_DIR
    output_file = f"{output_dir}/{req.test_case_id}.png"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    clean_old_images(output_dir)
    task_id = req.test_case_id  # ç›´æ¥ç”¨ test_case_id åšä»»åŠ¡ ID
    task_info = {
        "status": "pending",
        "result_url": None,
        "detail": None,
        "output_file": output_file
    }
    async with task_lock:
        tasks[task_id] = task_info

    async def do_generate():
        try:
            cpu_offload = check_cuda_offload()
            async with pipe_lock:
                async with task_lock:
                    tasks[task_id]["status"] = "running"
                print(f"ğŸ”„ Generating image for {task_id} to {output_file}")
                await asyncio.get_event_loop().run_in_executor(
                    None, _generate,
                    req.prompt, req.negative_prompt, req.aspect_ratio, req.num_inference_steps,
                    req.true_cfg_scale, req.seed, req.language,
                    output_file, cpu_offload, app.state.model_path
                )
                print(f"âœ… Image generated for {task_id} at {output_file}")
            async with task_lock:
                tasks[task_id]["status"] = "done"
                tasks[task_id]["result_url"] = f"/result/{req.test_case_id}.png"
        except Exception as e:
            print(f"âŒ Error generating image for {task_id}: {e}")
            async with task_lock:
                tasks[task_id]["status"] = "failed"
                tasks[task_id]["detail"] = str(e)

    background_tasks.add_task(do_generate)
    return {"task_id": task_id}

@app.post("/generate")
async def generate_image(req: GenerateRequest, background_tasks: BackgroundTasks):
    # ç¡®å®šè¾“å‡ºç›®å½•å’Œæ–‡ä»¶è·¯å¾„
    output_dir = req.output_dir if req.output_dir else DEFAULT_OUTPUT_DIR
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    clean_old_images(output_dir)
    task_id = str(uuid.uuid4())
    output_file = f"{output_dir}/{task_id}.png"
    
    task_info = {
        "status": "pending",
        "result_url": None,
        "detail": None,
        "callback_url": req.callback_url,
        "output_file": output_file
    }
    async with task_lock:
        tasks[task_id] = task_info
    
    async def do_generate():
        async with task_lock:
            tasks[task_id]["status"] = "pending"
        try:
            cpu_offload = check_cuda_offload()
            # pipeline ä¸²è¡Œé”!
            async with pipe_lock:
                async with task_lock:
                    tasks[task_id]["status"] = "running"
                print(f"ğŸ”„ Generating image for {task_id} to {output_file}")
                await asyncio.get_event_loop().run_in_executor(
                    None, _generate,
                    req.prompt, req.negative_prompt, req.aspect_ratio, req.num_inference_steps,
                    req.true_cfg_scale, req.seed, req.language,
                    output_file, cpu_offload, app.state.model_path
                )
                print(f"âœ… Image generated for {task_id} at {output_file}")
            async with task_lock:
                tasks[task_id]["status"] = "done"
                tasks[task_id]["result_url"] = f"/result/{task_id}.png"
            # å›è°ƒ
            if req.callback_url:
                try:
                    requests.post(req.callback_url, json={"task_id": task_id, "result_url": tasks[task_id]["result_url"]})
                except Exception as e:
                    print(f"Callback failed: {e}")
        except Exception as e:
            print(f"âŒ Error generating image for {task_id}: {e}")
            async with task_lock:
                tasks[task_id]["status"] = "failed"
                tasks[task_id]["detail"] = str(e)
    background_tasks.add_task(do_generate)
    return {"task_id": task_id}

###############################################################################
# API - 2. æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
###############################################################################
@app.get("/status/{task_id}", response_model=GenerateStatusResponse)
async def check_status(task_id: str):
    async with task_lock:
        info = tasks.get(task_id)
        if not info:
            raise HTTPException(status_code=404, detail="Task not found")
        return GenerateStatusResponse(
            status=info['status'],
            result_url=info['result_url'],
            detail=info.get('detail')
        )

###############################################################################
# API - 3. å›¾ç‰‡ä¸‹è½½
###############################################################################
@app.get("/result/{filename}")
async def get_result(filename: str):
    # é¦–å…ˆå°è¯•ä»é»˜è®¤ç›®å½•æŸ¥æ‰¾
    path = os.path.join(DEFAULT_OUTPUT_DIR, filename)
    if not os.path.isfile(path):
        # å¦‚æœé»˜è®¤ç›®å½•æ²¡æœ‰ï¼Œå°è¯•ä»ä»»åŠ¡ä¿¡æ¯ä¸­æŸ¥æ‰¾
        async with task_lock:
            for task_info in tasks.values():
                if task_info.get("output_file") and os.path.basename(task_info["output_file"]) == filename:
                    path = task_info["output_file"]
                    break
        
        if not os.path.isfile(path):
            raise HTTPException(status_code=404, detail="Image not found")
    
    return FileResponse(path, media_type="image/png")

###############################################################################
# API - 4. å¥åº·æ£€æŸ¥
###############################################################################
@app.get("/healthz")
async def health_check():
    async with task_lock:
        queue_length = sum(1 for t in tasks.values() if t["status"] in {"pending", "running"})
    return {
        "status": "ok",
        "queue_length": queue_length
    }

@app.get("/queue")
async def get_queue():
    """
    è¿”å›å½“å‰æ’é˜Ÿä¸­çš„ä»»åŠ¡ï¼ˆpending/runningï¼‰
    """
    async with task_lock:
        result = [ dict(task_id=k, **v) for k,v in tasks.items() if v['status'] in ('pending', 'running') ]
    return result

@app.get("/history")
async def get_history(skip: int = 0, limit: int = 30):
    """
    è¿”å›å·²å®Œæˆæˆ–å¤±è´¥çš„å†å²ä»»åŠ¡ï¼Œå¸¦åˆ†é¡µ
    """
    async with task_lock:
        # æŒ‰æ—¶é—´æ’åºï¼Œæœ€æ–°ä¼˜å…ˆ
        result = [ 
            dict(task_id=k, **v) 
            for k,v in tasks.items() if v['status'] in ('done','failed')
        ]
        # æ ¸å¿ƒä¿®æ”¹ï¼šè®¡ç®—ctime
        for x in result:
            if os.path.exists(x['output_file']):
                x['ctime'] = os.path.getctime(x['output_file'])
            else:
                x['ctime'] = None
        result.sort(key=lambda x: x['ctime'] or 0, reverse=True)
    return {
        "total": len(result),
        "items": result[skip:skip+limit]
    }

@app.get("/", include_in_schema=False)
async def root():
    return RedirectResponse(url="/static/index.html")

if __name__ == '__main__':
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=6006)